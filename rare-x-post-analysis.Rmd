---
title: "determine-top-performers-task2"
author: "Maria Diaz"
date: "2023-08-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
  
In order to declare top-performers for a DREAM challenge, we need to assess if there are any "tied" methods, that is, methods that are not substantially different in performance. We determine this by using a bootstrapping (sampling with replacement) approach to determining how a submission would score in different scenarios (that is - when only considering re-sampled sets of the values to be predicted). Specifically, we sample with replacement all of the submitted predictions and the goldstandard, then score those prediction files. We repeat this for at total of 1000-10000 samples to obtain a distribution of scores for each participant. We then calculate a Bayes factor relative to the best-scoring method, to see if any of the other methods are within a certain threshold. Smaller Bayes factors indicate more similar performance while larger Bayes factors indicate more disparate performance. We use a Bayes factor of 3 as a cutoff to indicate a tie. 

## Setup

First, we import the packages needed for data manipulation.  Afterward, we retrieve the predictions and goldstandard files, as well as set a seed (so that the resampling results are reproducible).

#### Packages

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(yardstick))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(reticulate))
suppressPackageStartupMessages(library(rcompanion))

reticulate::use_condaenv('synapse')
synapseclient <- reticulate::import('synapseclient')
syn <- synapseclient$Synapse()
syn$login(silent = TRUE)

set.seed(98109)
```

#### Helper Functions

Querying a Synapse table of submissions will return `submitterid` as either a user ID or team ID (so, an integer).  The following function will return a username or team name based on the ID, for easier identification and more comprehensible plotting. Additionally, `computeBayesFactor()` from the `challengescoring` package is outdated, so redefining an updated function here.


```{r echo=TRUE, message=FALSE, warning=FALSE}
get_name <- function(id) {
  name <- tryCatch({
    syn$getUserProfile(id)$userName
  }, error = function(err) {
    syn$getTeam(id)$name
  })
  name
}

computeBayesFactor <- function(bootstrapMetricMatrix,
                               refPredIndex,
                               invertBayes){
  
  M <- as.data.frame(bootstrapMetricMatrix - bootstrapMetricMatrix[,refPredIndex])
  K <- apply(M ,2, function(x) {
    k <- sum(x >= 0)/sum(x < 0)
    
    # Logic handles whether reference column is the best set of predictions.
    if(sum(x >= 0) > sum(x < 0)){
      return(k)
    }else{
      return(1/k)
    }
  })
  K[refPredIndex] <- 0
  if(invertBayes == T){K <- 1/K}
  return(K)
}
```

#### Goldstandard Files

```{r echo=TRUE, message=FALSE, warning=FALSE}
gold <- read_tsv(syn$get("syn52069273")$path)
#gold$validation <- factor(gold$was_preterm, levels=c(1,0))
head(gold)
```
#### Prediction Files

Participants are allowed 1 scored submissions, where we will only consider the best-performing one for final evaluation. Establishing a rank for each submission.

```{r echo=TRUE, message=FALSE, warning=FALSE}
query <- syn$tableQuery(
  "SELECT
    id,
    submitterid,
    MAX(accuracy) AS accuracy,
    prediction_fileid
  FROM syn52141680
  WHERE
    status = 'ACCEPTED'
    AND submission_status = 'SCORED'
    AND accuracy IS NOT NULL
  GROUP BY submitterid
  ORDER BY accuracy")$asDataFrame()

# Replace IDs with usernames/team names.
query$submitterid <- as.character(query$submitterid)
team_names <- sapply(query$submitterid, function(sub) {
  get_name(sub)
})
query$submitterid <- team_names 

# Drop row.names for easier table reading.
row.names(query) <- NULL

# Order the rows by highest accuracy
query <- query %>%
  arrange(desc(accuracy))

# add primary rank column for future Bayes analysis
query$primary_rank = seq(1, by = 1, length.out = nrow(query))

kable(query)
```
## Bootstrap Submissions

Next, we read in the predictions files and combine them together (with the goldstandard) into a single data frame. This will make bootstrapping easier.

```{r echo=TRUE, message=FALSE, warning=FALSE}
pred_filenames <- lapply(query$prediction_fileid, function(id){
  syn$get(id)$path
})
names(pred_filenames) <- team_names

submissions <- lapply(names(pred_filenames), function(team) {
  read_tsv(pred_filenames[[team]]) %>%
    select(Participant_ID, Disease_Name) %>% 
    rename(!!team := Disease_Name) 
}) %>% 
  purrr::reduce(left_join, by="Participant_ID") %>%
  slice(match(gold$Participant_ID, Participant_ID)) %>%
  left_join(gold, by="Participant_ID")   

#names(submissions)[names(submissions) == "Disease_Name"] <- "gold_truth"

kable(head(submissions))
```

Now we will bootstrap the predictions and the goldstandard 1 time to confirm that the results are as expected.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
N <- 1
bs_indices <- matrix(1:nrow(gold), nrow(gold), N) %>%
  apply(2, sample, replace = TRUE)

boot <- sapply(names(pred_filenames), function(team){
  framed <- as.vector(submissions[team])
  gt<- gold$Disease_Name
  #accuracy(gt,framed)
  #class(gt)
  apply(bs_indices, 2, function(ind) {
    
    #accuracy(gt,framed)
    #submissions[team] <- ifelse(submissions[team] == submissions$gold_truth, 1, 0)
    #framed
    #accuracy(submissions,factor(submissions$gold_truth),factor(framed))
    #accuracy(submissions,submissions$gold_truth,submissions[team])
    #obs <- sum(submissions[team])
    #true_val <- colSums(submissions[team]==0)
    #acc <- (true_val /(true_val + obs) * 100)
    #acc
    #gt(submissions$gold_truth,submissions$team)
    gt
    
  })
})

boot
```

Now we will bootstrap the predictions and the goldstandard 1000 times.  This will produce a matrix of 1000 scores per submission.

```{r echo=TRUE, message=FALSE, warning=FALSE}
N <- 1000
bs_indices <- matrix(1:nrow(gold), nrow(gold), N) %>%
  apply(2, sample, replace = TRUE)

boot <- sapply(names(pred_filenames), function(team){
  apply(bs_indices, 2, function(ind) {
    roc_auc_vec(submissions$gold[ind], submissions[[team]][ind])
  })
})
```