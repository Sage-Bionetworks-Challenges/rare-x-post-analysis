---
title: "determine-top-performers-task2"
author: "Maria Diaz"
date: "2023-08-30"
output: html_document
---


## Overview
  
In order to declare top-performers for a DREAM challenge, we need to assess if there are any "tied" methods, that is, methods that are not substantially different in performance. We determine this by using a bootstrapping (sampling with replacement) approach to determining how a submission would score in different scenarios (that is - when only considering re-sampled sets of the values to be predicted). Specifically, we sample with replacement all of the submitted predictions and the goldstandard, then score those prediction files. We repeat this for at total of 1000-10000 samples to obtain a distribution of scores for each participant. We then calculate a Bayes factor relative to the best-scoring method, to see if any of the other methods are within a certain threshold. Smaller Bayes factors indicate more similar performance while larger Bayes factors indicate more disparate performance. We use a Bayes factor of 3 as a cutoff to indicate a tie. 

## Setup

First, we import the packages needed for data manipulation.  Afterward, we retrieve the predictions and goldstandard files, as well as set a seed (so that the resampling results are reproducible).

#### Packages

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(yardstick))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(rcompanion))

#reticulate::use_condaenv('synapse')
synapseclient <- reticulate::import('synapseclient')
syn <- synapseclient$Synapse()
syn$login(silent = TRUE)

set.seed(98109)
```

#### Helper Functions

Querying a Synapse table of submissions will return `submitterid` as either a user ID or team ID (so, an integer).  The following function will return a username or team name based on the ID, for easier identification and more comprehensible plotting. Additionally, `computeBayesFactor()` from the `challengescoring` package is outdated, so redefining an updated function here.


```{r echo=TRUE, message=FALSE, warning=FALSE}
get_name <- function(id) {
  name <- tryCatch({
    syn$getUserProfile(id)$userName
  }, error = function(err) {
    syn$getTeam(id)$name
  })
  name
}

computeBayesFactor <- function(bootstrapMetricMatrix,
                               refPredIndex,
                               invertBayes){
  
  M <- as.data.frame(bootstrapMetricMatrix - bootstrapMetricMatrix[,refPredIndex])
  K <- apply(M ,2, function(x) {
    k <- sum(x >= 0)/sum(x < 0)
    
    # Logic handles whether reference column is the best set of predictions.
    if(sum(x >= 0) > sum(x < 0)){
      return(k)
    }else{
      return(1/k)
    }
  })
  K[refPredIndex] <- 0
  if(invertBayes == T){K <- 1/K}
  return(K)
}
```

#### Goldstandard Files
Indicate 1 as the validation column value since all values are correct.

```{r echo=TRUE, message=FALSE, warning=FALSE}
gold <- read_tsv(syn$get("syn52069273")$path)
gold$validation <- as.factor(1)
head(gold)
```
#### Prediction Files

Participants are allowed 1 scored submissions, where we will only consider the best-performing one for final evaluation. Establishing a rank for each submission.

AND a.prediction_fileid == b.prediction_fileid
    GROUP BY submitterid, accuracy
    ORDER BY accuracy

```{r echo=TRUE, message=FALSE, warning=FALSE}
query <- syn$tableQuery(
  "SELECT
    submitterid,
    accuracy,
    prediction_fileid
  FROM syn52141680
  WHERE
    status = 'ACCEPTED'
    AND submission_status = 'SCORED'
    AND accuracy IS NOT NULL
    GROUP BY submitterid,prediction_fileid
    ORDER BY MAX(accuracy) DESC
  ")$asDataFrame()

query <- query %>%
  distinct(submitterid, .keep_all = TRUE)

# Replace IDs with usernames/team names.
query$submitterid <- as.character(query$submitterid)
team_names <- sapply(query$submitterid, function(sub) {
  get_name(sub)
})

query$submitterid <- team_names 
# Drop row.names for easier table reading.
row.names(query) <- NULL

kable(query)

```
## Bootstrap Submissions

Next, we read in the predictions files and combine them together (with the goldstandard) into a single data frame. This will make bootstrapping easier.

```{r echo=TRUE, message=FALSE, warning=FALSE}

pred_filenames <- lapply(query$prediction_fileid, function(id){
  syn$get(id)$path
})
names(pred_filenames) <- team_names

submissions <- lapply(names(pred_filenames), function(team) {
  read_tsv(pred_filenames[[team]]) %>%
    select(Participant_ID, Disease_Name) %>% 
    rename(!!team := Disease_Name) 
}) %>% 
  purrr::reduce(left_join, by="Participant_ID") %>%
  slice(match(gold$Participant_ID, Participant_ID)) %>%
  left_join(gold, by="Participant_ID") %>%
  rename(gold = Disease_Name)   

kable(head(submissions))
```



Now we will bootstrap the predictions and the goldstandard 1 time to confirm that the results are as expected. The scoring function is taken directly from the original score code and has been rewritten in R.

```{r echo=TRUE}
#N <- 1
#bs_indices <- matrix(1:nrow(gold), nrow(gold), N) %>%
  #apply(2, sample, replace = TRUE)

#boot_test <- sapply(names(pred_filenames), function(team){
  #apply(bs_indices, 2, function(ind) {
    #sum(submissions$gold == submissions[[team]]) / nrow(gold)
  #})
#})
```

```{r}
#boot_test
```
Now we will bootstrap the predictions and the goldstandard 1000 times.  This will produce a matrix of 1000 scores per submission.

```{r echo=TRUE, message=FALSE, warning=FALSE}
N <- 1 #000

bs_indices <- matrix(1:nrow(gold), nrow(gold), N) 


#golden
boot <- sapply(names(append(pred_filenames,'gold')), function(team){
  new_col <- "pred_"
  pred_filenames
  apply(bs_indices, 2, function(ind) {
    submissions$gold[ind]
    # add binary prediction values 1== CORRECT
    final_col <- paste(new_col, team)
    #names(pred_filenames) <- final_col
    #team
    as.factor(ifelse(submissions['gold'] == submissions[[team]][ind], 1, 0))
    accuracy_vec(factor(submissions['validation']),factor(ifelse(submissions['gold'] == submissions[[team]][ind], 1, 0)))
    #sum(submissions['gold'] == submissions[[team]][ind]) / sum(submissions['validation'] == 1)
    #final <- submissions[[team]][ind]
    #return(final)
    #submissions['validation']
    #confusionMatrix(submissions$validation,final)
  })
  #submissions$team
  
})
#submissions$team
```

```{r echo=TRUE}
boot
```

## Compute and Plot Bayes Factor

For this analysis, we will use the top-performing model as the reference prediction. As a reminder, we will use a Bayes factor of 3 as a cutoff to indicate a tie.

```{r echo=TRUE, message=FALSE, warning=FALSE}
bayes <- computeBayesFactor(boot, refPredIndex = 2, invertBayes = FALSE) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)
```

## Comparing Models to the Baseline 

One of the requirements to being a top-performer is that it must do better than the baseline - it cannot be tied.  To analyze this, we will now use the baseline model as the reference prediction when computing the Bayes factor.

```{r echo=TRUE, message=FALSE, warning=FALSE}
bayes.ref <- computeBayesFactor(boot, refPredIndex = 29, invertBayes = FALSE) %>%
  as_tibble(rownames = "submission") %>%
  rename(bayes = value)
```


## Plot It All Together

```{r echo=FALSE, message=FALSE}
plot.auc <- boot %>%
  as_tibble() %>%
  tidyr::gather(submission, bs_score) %>%
  left_join(bayes) %>%
  mutate(submission = case_when(
    submission == "Preterm Birth - Vaginal Microbiome Organizers" ~ "Challenge Organizers",
    submission == "Aagaard Lab - Baylor College of Medicine - Texas Childrens Hospital " ~ "Aagaard Lab - BCM - Texas Childrens",
    submission == "Benos Team for DREAM challenge on Preterm Birth" ~ "Benos Team",
    TRUE ~ submission
 )) %>%
  mutate(bayes_category=case_when(
    submission == "Challenge Organizers" ~ "Baseline Model",
    submission == "AI4knowledgeLAB" ~ "Top Performers",
    bayes == 0 ~ "Top Performers",
    bayes <= 20 ~ "Bayes Factor ≤20",
    bayes >= 20 ~ "Bayes Factor >20")) %>%
  ggplot(aes(
    x = forcats::fct_reorder(submission, bs_score, .fun = mean),
    y = bs_score,
    color = bayes_category
  )) +
  geom_boxplot(lwd = 1.2, fatten = 1) +
  theme_bw() +
  scale_color_manual(values = c(
    "Top Performers" = "#FFBF00", 
    'Bayes Factor ≤20' = '#219EE6', 
    "Bayes Factor >20" = "#B6B5B3",
    "Baseline Model" = "#C74EDA"),
    name = NULL) +
  coord_flip() +
  labs(x="Team", y="Bootstrapped AUC-ROC\n(n=1000)") +
  theme(
    axis.text.y.left = element_text(size = 19),
    axis.text.x.bottom = element_text(size = 18),
    text = element_text(size = 16),
    legend.text = element_text(size = 19),
    legend.position = c(0.15, 0.92),
    legend.background = element_rect(linetype = "solid", color = "black"))

plot.bayes.top <- bayes %>% 
  mutate(bayes_category=case_when(
    submission == "Preterm Birth - Vaginal Microbiome Organizers" ~ "Baseline Model",
    submission == "AI4knowledgeLAB" ~ "Top Performer",
    bayes == 0 ~ "Top Performer",
    bayes <= 20 ~ "Bayes Factor ≤20",
    bayes >= 20 ~ "Bayes Factor >20")) %>% 
  ggplot(aes(submission, bayes, fill=bayes_category)) + 
  geom_bar(stat='identity') + coord_flip(ylim = c(0, 20)) +
  geom_hline(yintercept = 3, linetype = 2, lwd = 1.2) +
  theme_classic() + 
  scale_x_discrete(limits=names(sort(colMeans(boot)))) + 
  scale_fill_manual(values = c(
    "Top Performer" = "#FFBF00", 
    'Bayes Factor ≤20' = '#219EE6', 
    "Bayes Factor >20" = "#B6B5B3",
    "Baseline Model" = "#C74EDA")) +
  theme(legend.position = "none") +
  theme(
    text = element_text(size = 16),
    axis.text.x.bottom = element_text(size = 18),
    axis.title.y=element_blank(), 
    axis.text.y=element_blank()) + 
  labs(y="Bayes Factor\n(Top Performer=UWisc-Madison)")

plot.bayes.ref <- bayes.ref %>% 
  mutate(bayes_category=case_when(
    submission %in% c("UWisc-Madison", "AI4knowledgeLAB") ~ "Top Performer",
    bayes == 0 ~ "Baseline Model",
    bayes<=20 ~ "Bayes Factor ≤20",
    bayes>=20 ~ "Bayes Factor >20")) %>% 
  ggplot(aes(submission, bayes, fill=bayes_category)) + 
  geom_bar(stat='identity') + coord_flip(ylim = c(0, 20)) + 
  geom_hline(yintercept = 3, linetype = 2, lwd = 1.2) +
  theme_classic() + 
  scale_x_discrete(limits=names(sort(colMeans(boot)))) + 
  scale_fill_manual(values = c(
    "Top Performer" = "#FFBF00", 
    'Bayes Factor ≤20' = '#219EE6', 
    "Bayes Factor >20" = "#B6B5B3",
    "Baseline Model" = "#C74EDA")) +
  theme(legend.position = "none") +
  theme(
    text = element_text(size = 16),
    axis.text.x.bottom = element_text(size = 18),
    axis.title.y=element_blank(), 
    axis.text.y=element_blank()) + 
  labs(y="Bayes Factor\n(Baseline Model)")

ggsave(
  file="sc1-bf.svg",
  plot=gridExtra::grid.arrange(plot.auc, plot.bayes.top, plot.bayes.ref, ncol = 3, widths = c(3, 1, 1)),
  width = 24,
  height = 18.6
)
```
